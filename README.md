This project explores machine learning techniques, using the YOLOv8 instance segmentation model. The primary aim of this project is to provide users with an easy and convenient way to access detailed nutritional information about their meals without the need for manual input of the mealâ€™s contents. By simply snapping a photo of their meal, users can obtain valuable insights into the percentage of various macronutrients present on their plate, such as fiber, sugar, fat, protein, and carbohydrates.  

Instance segmentation is a technique used to detect the exact boundaries of an object, as opposed to traditional object detection, which only predicts a bounding box. The research investigates the complexity of the architecture used for instance segmentation, a network known as a convolutional neural network (CNN). In CNN, the process begins with pooling, which downsamples the image, followed by convolutional layers that extract different features of the image, like edges and colors, using filters. These features are then upsampled to increase resolution. Fully connected layers analyze these features to look for high-level patterns and eventually output which class the output belongs in. The coordinates of the features that make up the object are kept track of, which allows for the segmentation mask, which is an outline of the object, to be generated. The parameters in the functions used to analyze features are optimized during training epochs (cycles). The model is trained on a large dataset of annotated and labeled images, where its parameters are changed slightly during each epoch. 

Through rigorous experimentation and analysis, four distinct models were trained with varying numbers of epochs. The metrics of each model, segmentation loss, classification loss, focal loss, precision, and recall, were used to determine which of the 4 is the best-performing model. The 1st model was trained for 100 epochs, and the 2nd for 200 epochs. The 3rd was trained for 100 epochs, stopped, and then trained for another 50 epochs (100+50 epochs). The 4th was trained for 100+100 epochs. Stopping halfway through training helps prevent overfitting. Results show that model 1 had an especially high classification loss, and that the model has low precision when analyzing partially hidden objects, indicating that this model is underfitting. Model 2 was overfit, given the large difference between training and validation losses, and the fact that validation losses were increasing instead of decreasing during the last 100 epochs. Models 3 and 4 were similar in performance, but model 3 had slightly lower classification loss and higher precision. So, model 3, trained for 100+50 epoch, was selected as the best-performing model. 

The trained model is combined with Edamam Nutrition Analysis API, which gives accurate and up-to-date nutritional information regarding various food items. The nutrient percentage in each food item, adjusted for its size relative to the total size of the meal, is summed to determine the overall percentage content for each nutrient in the meal. The final software created for this project can analyze an image of a meal, and then output the nutrient percentage content of each food item and the overall meal.

Metrics of the model used during last 50 epochs of training:
<img src="C:\Users\danie_gfshjqx\Downloads\segoutput.jpg" width="200" height="200"/>

Sample output of the model:
![segoutput](https://github.com/Daniel-1-2-3/YOLOv8-Nutrition-Analysis/assets/144050857/8612af14-a219-4277-b555-e0949dc3375e)![Output](https://github.com/Daniel-1-2-3/YOLOv8-Nutrition-Analysis/assets/144050857/fff3983c-2684-4ea6-8545-b71e99be59bc)


